!pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master('local[*]') \
    .appName("Regressão com Spark") \
    .getOrCreate()

spark

#tratamento dos dados 

from google.colab import drive
drive.mount('/content/drive')


dados = spark.read.json(
    '/content/drive/MyDrive/Alura/imoveis.json'
)
dados

dados.show(truncate=False)

dados.count()

dados.printSchema()

dados\
    .select('ident.customerID', 'listing.*')\
    .show(truncate=False)
dados\
    .select('ident.customerID', 'listing.types.*', 'listing.features.*', 'listing.address.*', 'listing.prices.price', 'listing.prices.tax.*')\
    .show(truncate=False)

dados\
    .select('ident.customerID', 'listing.types.*', 'listing.features.*', 'listing.address.*', 'listing.prices.price', 'listing.prices.tax.*')\
    .drop('city', 'location', 'totalAreas')\
    .show(truncate=False)

dataset = dados\
    .select('ident.customerID', 'listing.types.*', 'listing.features.*', 'listing.address.*', 'listing.prices.price', 'listing.prices.tax.*')\
    .drop('city', 'location', 'totalAreas')
dataset.printSchema()

from pyspark.sql.types import IntegerType, DoubleType

dataset\
    .withColumn('usableAreas', dataset['usableAreas'].cast(IntegerType()))\
    .withColumn('price', dataset['price'].cast(DoubleType()))\
    .withColumn('condo', dataset['condo'].cast(DoubleType()))\
    .withColumn('iptu', dataset['iptu'].cast(DoubleType()))\
    .printSchema()

dataset = dataset\
    .withColumn('usableAreas', dataset['usableAreas'].cast(IntegerType()))\
    .withColumn('price', dataset['price'].cast(DoubleType()))\
    .withColumn('condo', dataset['condo'].cast(DoubleType()))\
    .withColumn('iptu', dataset['iptu'].cast(DoubleType()))

dataset.show()

dataset\
    .select('usage')\
    .groupBy('usage')\
    .count()\
    .show()

dataset = dataset\
    .select('*')\
    .where('usage=="Residencial"')

dataset\
    .select('unit')\
    .groupBy('unit')\
    .count()\
    .show()

dataset\
    .select('zone')\
    .groupBy('zone')\
    .count()\
    .show()

from pyspark.sql import functions as f

dataset\
    .select([f.count(f.when(f.isnan(c) | f.isnull(c), True)).alias(c) for c in dataset.columns])\
    .show()

dataset\
    .select('*')\
    .na\
    .fill(0)\
    .show()
dataset = dataset\
    .select('*')\
    .na\
    .fill(0)

dataset\
    .select([f.count(f.when(f.isnan(c) | f.isnull(c), True)).alias(c) for c in dataset.columns])\
    .show()

dataset\
    .select('zone')\
    .groupBy('zone')\
    .count()\
    .show()

dataset = dataset\
    .where(f.col('zone') != '')


dataset\
    .groupBy("customerID")\
    .pivot('unit')\
    .agg(f.lit(1))\
    .na\
    .fill(0)\
    .show()

unit = dataset\
    .groupBy('customerID')\
    .pivot('unit')\
    .agg(f.lit(1))\
    .na\
    .fill(0)

zone = dataset\
    .groupBy('customerID')\
    .pivot('zone')\
    .agg(f.lit(1))\
    .na\
    .fill(0)

dataset = dataset\
    .join(unit, 'customerID', how='inner')\
    .join(zone, 'customerID', how='inner')

dataset.show()

#Vectorizando valores

from pyspark.ml.feature import VectorAssembler

dataset = dataset.withColumnRenamed('price','label')

X = [
    'bathrooms',
    'bedrooms',
    'floors',
    'parkingSpaces',
    'suites',
    'unitFloor',
    'unitsOnTheFloor',
    'usableAreas',
    'condo',
    'iptu',
    'Apartamento',
    'Casa',
    'Outros',
    'Zona Central',
    'Zona Norte',
    'Zona Oeste',
    'Zona Sul'
]

assembler = VectorAssembler(inputCols = X, outputCol = 'features')

dataset_prep = assembler.transform(dataset).select('features', 'label')
dataset_prep.show(10, truncate=False)

#aplicando modelo

from pyspark.ml.stat import Correlation
import pandas as pd

correlacao = Correlation.corr(dataset_prep, 'features').collect()

correlacao = Correlation.corr(dataset_prep, 'features').collect()[0][0]

correlacao

dataframe_correlacao = pd.DataFrame(correlacao.toArray(), columns=X, index=X)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,10))
paleta = sns.color_palette("light:salmon", as_cmap=True)
sns.heatmap(dataframe_correlacao.round(1), annot=True, cmap=paleta)

from pyspark.ml.regression import LinearRegression

treino, teste = dataset_prep.randomSplit([0.7, 0.3], seed=101)

treino.count()

teste.count()

lr = LinearRegression()

modelo_lr = lr.fit(treino)

previsoes_lr_treino = modelo_lr.transform(treino)

previsoes_lr_treino.show()

resumo_treino = modelo_lr.summary

resumo_treino.r2

resumo_treino.rootMeanSquaredError

resumo_teste = modelo_lr.evaluate(teste)

resumo_teste.r2

resumo_teste.rootMeanSquaredError
print('Linear Regression')
print("="*30)
print("Dados de Treino")
print("="*30)
print("R²: %f" % resumo_treino.r2)
print("RMSE: %f" % resumo_treino.rootMeanSquaredError)
print("")
print("="*30)
print("Dados de Teste")
print("="*30)
print("R²: %f" % resumo_teste.r2)
print("RMSE: %f" % resumo_teste.rootMeanSquaredError)

from pyspark.ml.regression import DecisionTreeRegressor
dtr = DecisionTreeRegressor(seed=101, maxDepth=7)

modelo_dtr = dtr.fit(treino)

previsoes_dtr_treino = modelo_dtr.transform(treino)

previsoes_dtr_treino.show()

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator()

print(evaluator.evaluate(previsoes_dtr_treino, {evaluator.metricName: "r2"}))
print(evaluator.evaluate(previsoes_dtr_treino, {evaluator.metricName: "rmse"}))

previsoes_dtr_teste = modelo_dtr.transform(teste)

previsoes_dtr_teste.show()

print('Decision Tree Regression')
print("="*30)
print("Dados de Treino")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_dtr_treino, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_dtr_treino, {evaluator.metricName: "rmse"}))
print("")
print("="*30)
print("Dados de Teste")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_dtr_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_dtr_teste, {evaluator.metricName: "rmse"}))

from pyspark.ml.regression import RandomForestRegressor

rfr = RandomForestRegressor(seed=101, maxDepth=7, numTrees=10)

modelo_rfr = rfr.fit(treino)

previsoes_rfr_treino = modelo_rfr.transform(treino)

previsoes_rfr_treino.show()

print(evaluator.evaluate(previsoes_rfr_treino, {evaluator.metricName: "r2"}))
print(evaluator.evaluate(previsoes_rfr_treino, {evaluator.metricName: "rmse"}))

previsoes_rfr_teste = modelo_rfr.transform(teste)
previsoes_rfr_teste.show()

print('Random Forest Regression')
print("="*30)
print("Dados de Treino")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_rfr_treino, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_rfr_treino, {evaluator.metricName: "rmse"}))
print("")
print("="*30)
print("Dados de Teste")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_rfr_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_rfr_teste, {evaluator.metricName: "rmse"}))

from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import RegressionEvaluator

dtr = DecisionTreeRegressor()

grid = ParamGridBuilder() \
    .addGrid(dtr.maxDepth, [2, 5, 10]) \
    .addGrid(dtr.maxBins, [10, 32, 45]) \
    .build()

evaluator = RegressionEvaluator()

dtr_cv = CrossValidator(
    estimator=dtr,
    estimatorParamMaps=grid,
    evaluator=evaluator,
    numFolds=3,
    seed = 101
    )

modelo_dtr_cv = dtr_cv.fit(treino)

previsoes_dtr_cv_teste = modelo_dtr_cv.transform(teste)

print('Árvore de Decisão')
print("="*30)
print("Sem Cross Validation")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_dtr_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_dtr_teste, {evaluator.metricName: "rmse"}))
print("")
print("="*30)
print("Com Cross Validation")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_dtr_cv_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_dtr_cv_teste, {evaluator.metricName: "rmse"}))


from pyspark.ml.regression import RandomForestRegressor

rfr = RandomForestRegressor()

grid = ParamGridBuilder() \
    .addGrid(rfr.numTrees, [10, 20, 30]) \
    .addGrid(rfr.maxDepth, [5, 10]) \
    .addGrid(rfr.maxBins, [10, 32, 45]) \
    .build()

evaluator = RegressionEvaluator()

rfr_cv = CrossValidator(
    estimator=rfr,
    estimatorParamMaps=grid,
    evaluator=evaluator,
    numFolds=3
)

modelo_rfr_cv = rfr_cv.fit(treino)

previsoes_rfr_cv_teste = modelo_rfr_cv.transform(teste)

print('Random Forest')
print("="*30)
print("Sem Cross Validation")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_rfr_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_rfr_teste, {evaluator.metricName: "rmse"}))
print("")
print("="*30)
print("Com Cross Validation")
print("="*30)
print("R²: %f" % evaluator.evaluate(previsoes_rfr_cv_teste, {evaluator.metricName: "r2"}))
print("RMSE: %f" % evaluator.evaluate(previsoes_rfr_cv_teste, {evaluator.metricName: "rmse"}))
